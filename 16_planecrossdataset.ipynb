{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npiloting new script for cross dataset separating out the planes of ABA plus Cantin ST\\nmodified from: allenadultmouseISH/allbyall_planecrossdataset.py, allenadultmouseISH/crossplane_sixteen.ipynb and spatial/10_crossdataset.py\\n\\n\\nShaina Lu\\nZador + Gillis Labs\\nMay 2020\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "piloting new script for cross dataset separating out the planes of ABA plus Cantin ST\n",
    "modified from: allenadultmouseISH/allbyall_planecrossdataset.py, allenadultmouseISH/crossplane_sixteen.ipynb and spatial/10_crossdataset.py\n",
    "\n",
    "\n",
    "Shaina Lu\n",
    "Zador + Gillis Labs\n",
    "May 2020\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split #stratify train/test split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths\n",
    "ALLEN_FILT_PATH = \"/home/slu/spatial/data/ABAISH_filt_v6.h5\" #non-averaged version\n",
    "ONTOLOGY_PATH = \"/data/slu/allen_adult_mouse_ISH/ontologyABA.csv\"\n",
    "ST_CANTIN_FILT_PATH = \"/home/slu/spatial/data/cantin_ST_filt_v2.h5\"\n",
    "ALLEN_PLANES_PATH = '/home/slu/spatial/data/ABAplanes_v2.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_ABAdata():\n",
    "    \"\"\"read in all ABA datasets needed using pandas\"\"\"\n",
    "    metabrain = pd.read_hdf(ALLEN_FILT_PATH, key='metabrain', mode='r')\n",
    "    voxbrain = pd.read_hdf(ALLEN_FILT_PATH, key='voxbrain', mode='r')\n",
    "    propontvox = pd.read_hdf(ALLEN_FILT_PATH, key='propontology', mode='r')\n",
    "    geneIDName = pd.read_hdf(ALLEN_FILT_PATH, key='geneIDName', mode='r')\t\n",
    "\n",
    "    return metabrain, voxbrain, propontvox, geneIDName\n",
    "\n",
    "def read_STdata():\n",
    "    \"\"\"read in all ST datasets needed using pandas\"\"\"\n",
    "    STspotsmeta = pd.read_hdf(ST_CANTIN_FILT_PATH, key='STspotsmeta', mode='r')\n",
    "    STspots = pd.read_hdf(ST_CANTIN_FILT_PATH, key='STspots', mode='r')\n",
    "    STpropont = pd.read_hdf(ST_CANTIN_FILT_PATH, key='STpropont', mode='r')\n",
    "    \n",
    "    return STspotsmeta, STspots, STpropont\n",
    "\n",
    "def read_ontology():\n",
    "    ontology = pd.read_csv(ONTOLOGY_PATH)\n",
    "    ontology = ontology.drop([ontology.columns[5], ontology.columns[6]], axis=1)\n",
    "    ontology = ontology.fillna(-1)  #make root's parent -1\n",
    "\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "ontology = read_ontology()\n",
    "ABAmeta, ABAvox, ABApropont, geneIDName = read_ABAdata()\n",
    "STmeta, STspots, STpropont = read_STdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get duplicates across planes and write to hdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this box for subsequent runs and skip all stuff below in this section\n",
    "def read_ABAplanes():\n",
    "    sagvoxbrain = pd.read_hdf(ALLEN_PLANES_PATH, key='sagvoxbrain', mode='r')\n",
    "    corvoxbrain = pd.read_hdf(ALLEN_PLANES_PATH, key='corvoxbrain', mode='r')\n",
    "    \n",
    "    return sagvoxbrain, corvoxbrain\n",
    "\n",
    "sagvoxbrain, corvoxbrain = read_ABAplanes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__skip all below in subsequent runs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# planes outfile\n",
    "ALLEN_PLANES_PATH = 'ABAplanes_v2.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only gene symbols that are duplicated\n",
    "dups = geneIDName.duplicated(subset=\"gene_symbol\", keep=False) #marks all duplicates as True\n",
    "geneIDName_dups = geneIDName.loc[dups,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out sagittal and coronal planes\n",
    "sagittal = geneIDName_dups.loc[geneIDName_dups.plane == \"sagittal\", :]\n",
    "coronal = geneIDName_dups.loc[geneIDName_dups.plane == \"coronal\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average gene symbols that are duplicated within a plane\n",
    "#get duplicated gene symbols for each plane\n",
    "sagdupsfilt = sagittal.duplicated(subset=\"gene_symbol\", keep=False) #sets all duplicates as True\n",
    "sagdups = sagittal.loc[sagdupsfilt,:]\n",
    "sagnodups = sagittal.loc[~sagdupsfilt, :]\n",
    "\n",
    "cordupsfilt = coronal.duplicated(subset=\"gene_symbol\", keep=False)\n",
    "cordups = coronal.loc[cordupsfilt,:]\n",
    "cornodups = coronal.loc[~cordupsfilt, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagedups(nodupgenes, dupgenes, voxbrain):\n",
    "    \"\"\"average genes that are duplicated in a given dataframe\n",
    "       inputs: nodupgenes= genes that are not duplicated w/in plane,\n",
    "               dupgenes= geneIDName table only for duplicated genes, voxbrain= full dataset\"\"\"\n",
    "    #initialze avgvoxbrain with non-duplicated genes\n",
    "    indexers = nodupgenes.loc[nodupgenes.expt_id.isin(list(voxbrain)),\"expt_id\"] #solves NAN cols\n",
    "    avgvoxbrain = voxbrain.loc[:,indexers]\n",
    "    #rename the columns using gene_symbols\n",
    "    temp = dict(zip(nodupgenes.expt_id.tolist(),nodupgenes.gene_symbol.tolist()))\n",
    "    avgvoxbrain.rename(columns=temp, inplace=True)\n",
    "    \n",
    "    #get unique gene_symbols from duplicated\n",
    "    uniqfilt = dupgenes.duplicated(subset=\"gene_symbol\", keep='first')\n",
    "    uniqgenes = dupgenes.loc[~uniqfilt,:]\n",
    "\n",
    "    for i in range(uniqgenes.shape[0]):  #loop through unique duplicated genes\n",
    "        #get all series ids for current uniq gene symbol\n",
    "        currseries = dupgenes.loc[dupgenes.gene_symbol == uniqgenes.iloc[i,1], \"expt_id\"].values\n",
    "        curravg = voxbrain.loc[:,currseries].mean(axis=1) #get voxels and average them\n",
    "        curravg = curravg.rename(uniqgenes.iloc[i,1])     #rename and add to df\n",
    "        avgvoxbrain = pd.concat([avgvoxbrain, curravg], axis=1)\n",
    "        \n",
    "    return avgvoxbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slu/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1404: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "#this took several mins to an hour on rugen 2\n",
    "sagvoxbrain = averagedups(sagnodups, sagdups, ABAvox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corvoxbrain = averagedups(cornodups, cordups, ABAvox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to hdf\n",
    "sagvoxbrain.to_hdf(ALLEN_PLANES_PATH, key=\"sagvoxbrain\", mode='w')\n",
    "corvoxbrain.to_hdf(ALLEN_PLANES_PATH, key=\"corvoxbrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filterproponto(sampleonto):\n",
    "    \"\"\"pre-processing for propogated ontology\"\"\"\n",
    "    #remove brain areas that don't have any samples\n",
    "    sampleonto_sums = sampleonto.apply(lambda col: col.sum(), axis=0)\n",
    "    sampleonto = sampleonto.loc[:,sampleonto_sums > 5] #greater than 5 becuase less is not enough for train/test split to have non-zero areas\n",
    "    \n",
    "    return sampleonto\n",
    "\n",
    "def getleaves(propontvox, ontology):\n",
    "    \"\"\"helper function to get only leaf brain areas\"\"\"\n",
    "    #leaves are brain areas in the ontology that never show up in the parent column\n",
    "    allareas = list(propontvox)\n",
    "    parents = list(ontology.parent)\n",
    "    for i in range(len(parents)): #convert parents from float to int, ids are ints\n",
    "        parents[i] = int(parents[i])\n",
    "    \n",
    "    #remove parents from all areas\n",
    "    leaves = []\n",
    "    for area in allareas:\n",
    "        if int(area) not in parents:\n",
    "            leaves.append(area)\n",
    "    \n",
    "    print(\"number of leaf areas: %d\" %len(leaves))\n",
    "    return leaves\n",
    "\n",
    "def findoverlapareas(STonto, propontvox, ontology):\n",
    "    \"\"\"find leaf brain areas overlapping between the two datasets\"\"\"\n",
    "    leafST = getleaves(STonto, ontology)\n",
    "    leafABA = getleaves(propontvox, ontology)\n",
    "\n",
    "    leafboth = [] \n",
    "    for i in range(len(leafABA)):\n",
    "        if leafABA[i] in leafST:\n",
    "            leafboth.append(leafABA[i])\n",
    "    \n",
    "    STonto = STonto.loc[:,leafboth]\n",
    "    propontvox = propontvox.loc[:,leafboth]\n",
    "    \n",
    "    return STonto, propontvox    \n",
    "\n",
    "def zscore(voxbrain):\n",
    "    \"\"\"zscore voxbrain or subsets of voxbrain (rows: voxels, cols: genes)\"\"\"\n",
    "    #z-score on whole data set before splitting into test and train\n",
    "    scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    scaler.fit(voxbrain)\n",
    "    z_voxbrain = scaler.transform(voxbrain)\n",
    "    \n",
    "    #store z-scored voxbrain as pandas dataframe\n",
    "    z_voxbrain = pd.DataFrame(z_voxbrain)\n",
    "    z_voxbrain.columns = voxbrain.columns\n",
    "    z_voxbrain.index = voxbrain.index\n",
    "    \n",
    "    return z_voxbrain\n",
    "\n",
    "def analytical_auroc(featurevector, binarylabels):\n",
    "    \"\"\"analytical calculation of auroc\n",
    "       inputs: feature (mean rank of expression level), binary label (ctxnotctx)\n",
    "       returns: auroc\n",
    "    \"\"\"\n",
    "    #sort ctxnotctx binary labels by mean rank, aescending\n",
    "    s = sorted(zip(featurevector, binarylabels))\n",
    "    feature_sort, binarylabels_sort = map(list, zip(*s))\n",
    "\n",
    "    #get the sum of the ranks in feature vector corresponding to 1's in binary vector\n",
    "    sumranks = 0\n",
    "    for i in range(len(binarylabels_sort)):\n",
    "        if binarylabels_sort[i] == 1:\n",
    "            sumranks = sumranks + feature_sort[i]\n",
    "    \n",
    "    poslabels = binarylabels.sum()\n",
    "    neglabels = (len(binarylabels) - poslabels)\n",
    "    \n",
    "    auroc = ((sumranks/(neglabels*poslabels)) - ((poslabels+1)/(2*neglabels)))\n",
    "    \n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is new from before, but same idea as sptial/10_crossdataset.py\n",
    "def getoverlapgenes(corvoxbrain,sagvoxbrain,STspots):\n",
    "    \"\"\"find genes that are present in all 3 datasets using gene symbol directly\"\"\"\n",
    "    corgenes = list(corvoxbrain)\n",
    "    saggenes = list(sagvoxbrain)\n",
    "    STgenes = list(STspots)\n",
    "\n",
    "    #getoverlapping genes\n",
    "    overlap = []\n",
    "    for i in range(len(STgenes)):\n",
    "        if STgenes[i] in corgenes:\n",
    "            if STgenes[i] in saggenes:\n",
    "                overlap.append(STgenes[i])\n",
    "\n",
    "    print(\"number of overlapping genes: %d\" %len(overlap))\n",
    "\n",
    "    #subset datasets to only keep genes that are overlapping\n",
    "    corvoxbrain = corvoxbrain.loc[:,overlap]\n",
    "    sagvoxbrain = sagvoxbrain.loc[:,overlap]\n",
    "    STspots = STspots.loc[:,overlap]\n",
    "\n",
    "    return corvoxbrain, sagvoxbrain, STspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyLASSO(Xtrain, Xtest, Xcross1, Xcross2, ytrain, ytest, ycross1, ycross2):\n",
    "    \"\"\"apply LASSO regression\"\"\"\n",
    "    lasso_reg = Lasso(alpha=0.05,max_iter=10000) #alpha=alphaval) #,max_iter=10000)\n",
    "    #lasso_reg = LinearRegression()\n",
    "    lasso_reg.fit(Xtrain, ytrain)\n",
    "    \n",
    "    #train\n",
    "    predictions_train = lasso_reg.predict(Xtrain)\n",
    "    auroc_train = analytical_auroc(sp.stats.mstats.rankdata(predictions_train), ytrain)\n",
    "    #auroc_train = metrics.roc_auc_score(y_true = ytrain, y_score = predictions_train)\n",
    "    #test\n",
    "    predictions_test = lasso_reg.predict(Xtest)\n",
    "    auroc_test = analytical_auroc(sp.stats.mstats.rankdata(predictions_test), ytest)\n",
    "    #auroc_test = metrics.roc_auc_score(y_true = ytest, y_score = predictions_test)\n",
    "    \n",
    "    #cross 1\n",
    "    predictions_cross1 = lasso_reg.predict(Xcross1)\n",
    "    auroc_cross1 = analytical_auroc(sp.stats.mstats.rankdata(predictions_cross1), ycross1)\n",
    "    \n",
    "    #cross 2\n",
    "    predictions_cross2 = lasso_reg.predict(Xcross2)\n",
    "    auroc_cross2 = analytical_auroc(sp.stats.mstats.rankdata(predictions_cross2), ycross2)\n",
    "    \n",
    "    return auroc_train, auroc_test, auroc_cross1, auroc_cross2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getallbyall(mod_data, mod_propont, cross1_data, cross1_propont, cross2_data, cross2_propont):\n",
    "    #initialize zeros dataframe to store entries\n",
    "    allbyall_selftest = pd.DataFrame(index=list(mod_propont), columns=list(mod_propont))\n",
    "    allbyall_selftrain = pd.DataFrame(index=list(mod_propont), columns=list(mod_propont))\n",
    "    allbyall_cross1 = pd.DataFrame(index=list(mod_propont), columns=list(mod_propont))\n",
    "    allbyall_cross2 = pd.DataFrame(index=list(mod_propont), columns=list(mod_propont))\n",
    "    \n",
    "    areas = list(mod_propont)\n",
    "    #for each column, brain area\n",
    "    for i in range(mod_propont.shape[1]):\n",
    "    #for i in range(5,6,1):\n",
    "        print(\"col %d\" %i)\n",
    "        #for each row in each column\n",
    "        for j in range(i+1,mod_propont.shape[1]): #upper triangular!\n",
    "            area1 = areas[i]\n",
    "            area2 = areas[j]\n",
    "            #get binary label vectors\n",
    "            ylabels = mod_propont.loc[mod_propont[area1]+mod_propont[area2] != 0, area1]\n",
    "            ycross1 = cross1_propont.loc[cross1_propont[area1]+cross1_propont[area2] != 0, area1]\n",
    "            ycross2 = cross2_propont.loc[cross2_propont[area1]+cross2_propont[area2] !=0, area1]\n",
    "            #ylabels = pd.Series(np.random.permutation(ylabels1),index=ylabels1.index) #try permuting\n",
    "            #subset train and test sets for only samples in the two areas\n",
    "            Xcurr = mod_data.loc[mod_propont[area1]+mod_propont[area2] != 0, :]\n",
    "            Xcross1curr = cross1_data.loc[cross1_propont[area1]+cross1_propont[area2] != 0, :]\n",
    "            Xcross2curr = cross2_data.loc[cross2_propont[area1]+cross2_propont[area2] != 0, :]\n",
    "            #split train test for X data and y labels\n",
    "            #split data function is seeded so all will split the same way\n",
    "            Xtrain, Xtest, ytrain, ytest = train_test_split(Xcurr, ylabels, test_size=0.5,\\\n",
    "                                                            random_state=42, shuffle=True,\\\n",
    "                                                            stratify=ylabels)\n",
    "            #z-score train and test folds\n",
    "            zXtrain = zscore(Xtrain)\n",
    "            zXtest = zscore(Xtest)\n",
    "            zXcross1 = zscore(Xcross1curr)\n",
    "            zXcross2 = zscore(Xcross2curr)\n",
    "            \n",
    "            currauroc_train, currauroc_test, currauroc_cross1, currauroc_cross2 = applyLASSO(zXtrain, zXtest, zXcross1, zXcross2, ytrain, ytest, ycross1, ycross2)\n",
    "            allbyall_selftrain.iloc[i,j] = currauroc_train\n",
    "            allbyall_selftest.iloc[i,j] = currauroc_test\n",
    "            allbyall_cross1.iloc[i,j] = currauroc_cross1\n",
    "            allbyall_cross2.iloc[i,j] = currauroc_cross2\n",
    "            #curr_row[0,j] = currauroc\n",
    "            \n",
    "        #if i == 1:\n",
    "        break\n",
    "    \n",
    "    #return temp\n",
    "    return allbyall_selftrain, allbyall_selftest, allbyall_cross1, allbyall_cross2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of leaf areas: 461\n",
      "number of leaf areas: 560\n"
     ]
    }
   ],
   "source": [
    "#filter brain areas for those that have at least x samples\n",
    "STpropont = filterproponto(STpropont)\n",
    "ABApropont = filterproponto(ABApropont)\n",
    "#filter brain areas for overlapping leaf areas\n",
    "STpropont, ABApropont = findoverlapareas(STpropont, ABApropont, ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of overlapping genes: 3737\n"
     ]
    }
   ],
   "source": [
    "#keep only genes that are overlapping between the datasets\n",
    "corvoxbrain, sagvoxbrain, STspots = getoverlapgenes(corvoxbrain, sagvoxbrain, STspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col 0\n"
     ]
    }
   ],
   "source": [
    "#predictability matrix using LASSO\n",
    "allbyall_train, allbyall_test, allbyall_cross1, allbyall_cross2 = getallbyall(STspots, STpropont, corvoxbrain, ABApropont, sagvoxbrain, ABApropont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allbyall_test.to_csv(ST_TEST_OUT, sep=',', header=True, index=False)\n",
    "allbyall_train.to_csv(ST_TRAIN_OUT, sep=',', header=True, index=False)\n",
    "allbyall_cross1.to_csv(COR_inST_ALL_OUT, sep=',', header=True, index=False)\n",
    "allbyall_cross2.to_csv(SAG_inST_ALL_OUT, sep=',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of zeros in each plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126032188"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sagvoxbrain == 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17432942"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(corvoxbrain == 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62527, 3737)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagvoxbrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62527, 3737)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corvoxbrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233663399"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "62527*3737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5393749664661859"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "126032188/233663399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07460707185895211"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17432942/233663399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
